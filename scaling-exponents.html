<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Scaling Exponent Hypothesis</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/source-sans-pro/3.046/source-sans-pro.min.css" rel="stylesheet">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;700&display=swap');

        :root {
            --background-dark: #1a1a1a;
            --background-lighter: #252525;
            --text-color: #ffffff;
            --text-secondary: rgba(255, 255, 255, 0.8);
            --border-color: rgba(255, 255, 255, 0.1);
            --max-width: 800px;
        }

        body {
            background-color: var(--background-dark);
            color: var(--text-color);
            font-family: 'Source Sans Pro', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 2rem;
        }

        blockquote {
            font-family: 'Source Serif Pro', serif;
            background-color: var(--background-lighter);
            border: 2px solid var(--border-color);
            padding: 1rem;
            margin-bottom: 1rem;
            border-radius: 4px;
            font-size: 1.1rem;
            line-height: 1.7;
        }

        .content li {
            font-family: 'Source Serif Pro', serif;
        }

        .container {
            max-width: var(--max-width);
            font-family: 'Source Sans Pro', sans-serif;
            margin: 0 auto;
        }

        .article-header {
            margin-bottom: 3rem;
            text-align: center;
        }

        h1 {
            font-family: 'Source Sans Pro', sans-serif;
            font-size: 2.5rem;
            font-weight: 600;
            margin-bottom: 2rem;
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
            justify-content: center;
            margin-bottom: 2rem;
            color: var(--text-secondary);
        }

        .tags span {
            font-style: italic;
        }

        .abstract {
            background-color: var(--background-lighter);
            border: 3px solid var(--border-color);
            font-family: 'Source Serif Pro', serif;
            padding: 2rem;
            margin-bottom: 3rem;
            border-radius: 4px;
            font-size: 1.1rem;
            line-height: 1.7;
        }

        p {
            font-family: 'Source Serif Pro', serif;
        }

        .metadata {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin-bottom: 2rem;
            text-align: center;
        }

        .metadata span {
            margin: 0 0.5rem;
        }

        .content {
            font-size: 1.1rem;
            line-height: 1.7;
        }

        .content h2 {
            font-size: 1.8rem;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
        }

        .content p {
            margin-bottom: 1.5rem;
        }

        a {
            color: var(--text-color);
            border-bottom: 1px dotted #FFF;
            text-decoration: none;
            transition: border-color 0.2s;
        }

        a:hover {
            border-bottom-color: var(--text-color);
        }

        .table-of-contents {
            background-color: var(--background-lighter);
            border: 3px solid var(--border-color);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .table-of-contents h2 {
            margin-top: 0;
            font-size: 1.4rem;
        }

        .table-of-contents ul {
            list-style: none;
            padding-left: 0;
        }

        .table-of-contents li {
            margin-bottom: 0.5rem;
        }

        .nav-container {
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            padding: 1rem 0;
            margin-bottom: 2rem;
        }

        .nav-list {
            list-style: none;
            padding: 0;
            margin: 0;
            display: flex;
            justify-content: center;
            gap: 2rem;
        }

        .nav-list a {
            color: #ffffff;
            text-decoration: none;
            font-family: 'Source Sans Pro', sans-serif;
            font-size: 1.1rem;
            transition: opacity 0.2s;
        }

        .nav-list a:hover {
            opacity: 0.8;
        }

        code {
          padding: 0.2em 0.4em;
          margin: 0;
          font-size: 85%;
          font-family: ui-monospace, SFMono-Regular, SF Mono, Menlo, Consolas, Liberation Mono, monospace;
          background-color: rgba(175, 184, 193, 0.2);
          border-radius: 6px;
          white-space: break-spaces;
        }
    </style>
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav-container">
            <ul class="nav-list">
                <li><a href="index.html">About</a></li>
                <li><a href="index.html">New</a></li>
                <li><a href="index.html">Archive</a></li>
            </ul>
        </nav>
        <article>
            <header class="article-header">
                <h1>The Scaling Exponent Hypothesis</h1>
                <div class="tags">
                    <span>agi forecasting</span> •
                    <span>ai scaling</span> • <span>growth curves</span>
                </div>
                <div class="metadata">
                    <span>2025-01-30 – 2025-??-??</span> •
                    <span>status: in-progress</span> •
                    <span>importance: ultra</span>
                    <span>certainty: lukewarm</span>
                </div>
            </header>

            <section class="abstract">
                Dario Amodei's <a href="https://darioamodei.com/on-deepseek-and-export-controls">response essay</a> to DeepSeek-v3 codifies new predictions from Anthropic (presumably mutual between its competitor labs). A hot zone is sketched around 2026-2027. Connecting the datapoints tells us AI progress projections from even the most informed have historically <b>underestimated</b> <i>algorithmic progress</i>. 
                <br><br>
                Taking into account the bias to expecting slow progress, and applying a "debias" of anticipating faster progress to Dario's 2026-27 human-level AI timeline, I revise my AGI timelines to 60% by EoY 2025.
                <br><br>
                At present, this document is rough speculation on little data. (<i>But so was <a href="https://www.chiphistory.org/chc_upload/content_inline_files/images/a18.jpg">Moore's Law</a></i>). Please send me sources that argue against this – I promise I will read them.
            </section>

            <div class="table-of-contents">
                <h2>Contents</h2>
                <ul>
                    <li>1. <a href="#1">Years Ahead but Way Behind</a></li>
                    <li>2. <a href="#2">Seeing With Eyes Unclouded</a></li>
                    <li>3. <a href="bibliography">Bibliography</a></li>
                </ul>
            </div> 
          
            <section class="content">
                <h2 id="1">1. Years Ahead but Way Behind</h2>
                <p>In &quot;<a href="https://darioamodei.com/on-deepseek-and-export-controls">On DeepSeek and Export Controls</a>&quot;, Dario Amodei reveals that their flagship model, Claude Sonnet 3.5, took on the order of &quot;$10s of millions&quot; to train, vastly cheaper than reports of GPT-4 ($50M — $200M), while also being 10 times cheaper to inference than its predecessor.</p>
                <p>This data point rests on a curve of such cost improvements: The training input, between the time, money, compute, and data required to train large models is based on a set of algorithms (from self-attention to MLP blocks in transformers to optimizations in GPU kernels that can allow those to run faster at a hardware level), and the output of those algorithms (seen in the performance quality of the trained models) is getting better over time.</p>
                <p>Per Dario&#39;s essay, little of this is public, but open source public improvements suggest an empirical curve. Compare training and inferencing a 1.5B parameter GPT-2 in 2019 to 2025, incorporating all these major improvements:</p>
                    <ul>
                        <li>Flash Attention reported <a href="https://arxiv.org/abs/2205.14135">a 3x improvement in training speed</a> for 2019&#39;s GPT-2 in 2022.</li>
                        <li>GPT-NeoX's <a href="https://arxiv.org/pdf/2204.06745#page=3">parallel attention and feedforward networks</a></li>
                    </ul>
            <code>to-do: complete this math</code>
                
                <p>Dario notes down that algorithmic progress <em>alone</em> is plowing forth at a pace of 4x gains per year today, meaning a model that takes $400m to train right now will take $100m to train next year, and $25m the year after that – and this trend is somewhere between 1.5x to 4x year-on-year since 2020.</p>
                <p>One of his source, an <a href="https://epoch.ai/blog/algorithmic-progress-in-language-models">Epoch AI progress report</a>, analyzes algorithmic progress data from 2014 onward to propose an inverse Moore&#39;s law: the total compute required to reach a level of performance <em>halves</em> every 8 months. This report, published in March of 2024, would <i>underestimate</i> where Dario pegs today's progress (an 8 month halving time is ~2.7x per year efficiency gain). Another source Dario cites is a <a href="https://cdn.openai.com/papers/ai_and_efficiency.pdf">2020 paper from OpenAI</a> that found progress from 2012 to 2019 to "double every 16 months".</p>
                <p>Plots of upward progress like this do not hold very often, and less often do they survive four years to end up <i>behind</i>.</p>
                <p>On the front of algorithmic progress, historical projections are biased to <em>underestimate</em> the growth of this single &quot;algorithmic progress&quot; coefficient. A &quot;meta&quot; predictor factoring in this bias would suggest that their projected performance for a training run in the $10s of billions will arrive <em>sooner</em> than their own expectations. So if Dario <a href="https://darioamodei.com/on-deepseek-and-export-controls">projects a run</a> in the “millions of chips” to result in models &quot;smarter than almost all humans at almost all things” to take place between 2026 and 2027, our meta-Dario, debiased to factor in the second-derivative of AI progress, would predict a run producing such capability to complete <i>even earlier</i>.</p>
                <!-- revise above paragraph so that zero fluff between sentences. i will write like gwern! -->
                <!--                 revise above paragarph. make my timeline prediction sound more sane, also more specific. I expect <i>what</i>. by EoY 2025? -->
                <p>This disrupts naive estimates of costs, and pulls into question well-informed ones based on the scaling laws of current models. What will take a 5GW datacenter now may take a sub-1GW datacenter in a few years. And if this continues, a training run of such performance may fall into the scope of existing clusters.</p>
                <p>Even if the actors at play are factoring the pace of progress into account, they don't seem to be accounting for their own prediction errors. On this basis, I forecast general human-level AI by the end-of-year 2025. To be specific, I expect that by the end of this year, a system will exist that is capable of drop-in replacing an average remote-work software engineer, autonomously conduct AI research at the level of the average "brilliant PhD" on its own, and make new discoveries on the level of drop-out or skip connections. A critical threshold will be passed where all further developments become the domain of AI systems as they are, a shock-wave knocking down all obstacles. What happens next? To echo DeepMind's <a href="https://en.wikipedia.org/wiki/Shane_Legg">Shane Legg</a> <a href="http://www.vetta.org/2009/08/funding-safe-agi/">in 2009</a>:</p>
                <blockquote>
                    Due to greed, wishful thinking, ignorance and what have you, in general safety will come second to progress.  A short period of time later the post human period will begin. 
                </blockquote>
                <code>to-do: make more specific, falsifiable predictions</code>
                <p>Human-level AI is much more plausible as a near-term project in the $10s of billions on this trajectory, not a massive war effort. To put this into relative terms, <a href="https://en.wikipedia.org/wiki/List_of_companies_by_research_and_development_spending">Apple&#39;s R&amp;D budget in 2022 was $27.65B alone</a>!</p>
                
                
<!--                 add evidence for this claim!!!     -->
                <h2 id="2">2. Seeing With Eyes Unclouded</h2>
                <blockquote>
                    <i>You must see with eyes unclouded by hate. See the good in that which is evil, and the evil in that which is good. Pledge yourself to neither side, but vow instead to preserve the balance that exists between the two.</i>
                    <br>
                    <br>
                    –<i>Princess Mononoke</i>, Hayao Miyazaki
                </blockquote>
                <code>Algorithmic progress of autoregressive transformers isn't the only factor at play, what new RL techniques might be doing in a Gwern-style <i>from the model's perspective</i></code>
                <br>
                <code>Better coding models rapidly explodes the potential architectural search space at our fingertips – this is one actual mechanism by which I believe that "4x improvement" figures will stay on track</code>

                <p>Timeline estimates are difficult. Being wrong about specifics doesn't necessarily mean being <i>wrong in spirit</i>. In 2011, Shane Legg <a href="http://www.vetta.org/2011/12/goodbye-2011-hello-2012/">predicted a "proto-AGI"</a> capable of learning primitive processing on its own by 2019, and this was arguably achieved by DeepMind's Gato in 2022. But Gato was just self-supervised learning on synthetic data from RL on video games! (Granted, stiched together with <a href="https://arxiv.org/abs/2010.11929">ViT</a> – which wouldn't be out until 2020 – to enable image processing, and a few other things tossed in, but nothing all-too special). 2019's GPT-2 was a fire-alarm alert that <i>unsupervised learning</i> could scale, arguably vindicating the <i>spirit</i> of Legg's prediction – that a system would exist capable of learning advanced representations from data on its own.</p>
                <blockquote>
                    I expect to see an impressive proto-AGI within the next 8 years.  By this I mean a system with basic vision, basic sound processing, basic movement control, and basic language abilities, with all of these things being essentially learnt rather than preprogrammed.  It will also be able to solve a range of simple problems, including novel ones.
                    <br><br>
                    –<i>Goodbye 2011, hello 2012</i>, Shane Legg
                </blockquote>
                <p>In the Scaling Hypothesis, Gwern reigns us back into historical perspective:</p>
                <blockquote>
                    There is, however, a certain tone of voice the bien pensant all speak in, whose sound is the same whether right or wrong; a tone shared with many statements in January to March of this year; a tone we can also find in a 1940 Scientific American article authoritatively titled, “Don’t Worry—It Can’t Happen”, which advised the reader to not be concerned about it any longer “and get sleep”. (‘It’ was the atomic bomb, about which certain scientists had stopped talking, raising public concerns; not only could it happen, the British bomb project had already begun, and 5 years later it did happen.)
                    <br><br>
                    –<i><a href="https://gwern.net/scaling-hypothesis">The Scaling Hypothesis</a></i>, Gwern Branwen
                </blockquote>
                <p>The Scaling Hypothesis was prior to any RL on language models actually working. Not really even RLHF. With o1 and RL-on-chain-of-thought showing that the limits of scaling decoder transformers on more data are surmountable with newer architectures or training regimes, with Dario's prophecy that dark Satanic Mills of millions of GPUs may soon spur to life and that the projected performance of a training run on such hardware being underestimated time and time again by the discovery of better "scaling exponents", and Ilya Sutskever retreating to <a href="https://ssi.inc">full research hermeticism</a>, the intimations in Gwern's original 2020 post are rapidly reaching fruition. It all gives the impression of some "critical threshold" just out of reach. Whether this is achieved in a year, six months, tommorrow, or next week isn't of much relevance.</p>
                <code>This section post should aim to identify the shadow of take-off looming, convincingly, with technical details about scaling exponents outside of unsupervised learning, such as RL on chain-of-thought</code>
                <blockquote>
                    Usually we are too wrapped up in our stories to catch even a glimpse of the gods. When we do, we can fight them, as Rachel Carson did after seeing disturbing trends in air and water pollution. Or we can ally with them, as Moore (and Kurzweil and Kaplan) did after seeing the exponential compute trends. But—ah, I hear the gods laughing again, in the face of these stories it’s always so tempting to tell. To the gods of straight lines, Carson and Moore did nothing, because the gods see (as we do not) the other timelines where the same insights came from other people, a month or a year or a decade delayed, but landing all the more powerfully because of it. The gods are intimately familiar with a fact that we can only hazily glimpse: that all great discoveries come in their natural time.
                    <br><br>
                    <i><a href="https://www.lesswrong.com/posts/xkRtegmqL2iyhtDB3/the-gods-of-straight-lines">The Gods of Straight Lines</a></i>, Richard Ngo
                </blockquote>
                <blockquote>
                    In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (OpenAI, 2024a; Anthropic, 2024; Google, 2024), progressively diminishing the gap towards Artificial General Intelligence (AGI).
                    <br><br>
                    –<i>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</i>, DeepSeek
                </blockquote>
                <hr>
                <h2 id="bibliography">Bibliography</h2>  
                <p>Gwern Branwen on <a href="https://www.reddit.com/r/mlscaling/comments/1f8uscr/comment/llj8suu/">so-called "Scaling Exponents"</a>:</p>
                <blockquote>
                  <p>Well, Ilya would know better what OA was doing under Ilya that led to Q*/Strawberry, and what SI is doing under Ilya now, and how they are different... As I still don't know what the former is, it is difficult for me to say what the latter might be.</p>
                  <p>In RL, minor input differences can lead to large output differences, to a much greater extent than in regular DL, so it can be hard to say how similar two approaches 'really' are. I will note that it seems like OA no longer has much DRL talent these days - even Schulman is gone now, remember - so there may not be much fingerspitzengefühl for 'RL' beyond preference-learning the way there used to be. (After all, if this stuff was so easy, why would anyone be giving Ilya the big bucks?)</p>
                  <p>If you get the scaling right and get a better exponent, you can scale way past the competition. This happens regularly, and you shouldn't be too surprised if it happened again. Remember, before missing the Transformer boat, Google was way ahead of everyone with n-grams too, training the largest n-gram models for machine translation etc, but that didn't matter once RNNs started working with a much better exponent and even a grad student or academic could produce a competitive NMT; they had to restart with RNNs like everyone else. (Incidentally, recall what Sutskever started with...)</p>
                </blockquote>

                <p>Stella Biderman on <a href="https://www.reddit.com/r/MachineLearning/comments/12yk3ea/comment/jhpp7jg/">improvements to the vanilla transformer architecture</a>:</p>
                <blockquote>
                    <p>There have been close to no improvements on the original transformer architecture; almost everything is a wash. The only real differences between current SOTA and the original paper are:</p>
                    <ol>
                        <li>You don’t have to use an encoder-decoder architecture, both decoder-only and encoder-only architectures are also useful. Different architecture are better at different tasks. Similar statements can be made about the training objective.</li>
                        <li>There’s a major error in the paper Attention is All You Need where they accidentally put the layer norms after the layers not before them. The impacts are laid out very well in <a href="https://arxiv.org/abs/2002.04745">this paper</a>. Note that the code for Attn is All You Need did it correctly, but nobody noticed and copied what is wrongly written in the paper.</li>
                        <li>Ben Wang at EleutherAI figured out that you can put attention layers and MLPs in parallel. This doesn’t really effect performance but makes the model run much faster. This was first introduced in the GPT-J-6B model and first described in a paper by <a href="https://arxiv.org/abs/2204.06745">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a>.</li>
                        <li>The original positional embedding method is garbage. Basically anything else is better, but Rotary Positional Embeddings are currently considered the mainstream way to do it. They’re basically Sinusoidal embedding but done correctly lol.</li>
                    </ol>
                    <p>That isn’t to say that there haven’t been some changes. Application paradigms and finetuning have especially changed… first domain-specific finetuning, then few shot prompting, then multitask finetuning and reinforcement learning from human feedback. But I do think it’s remarkable how much the OG paper got right and how little in the architecture has actually changed.</p>
                </blockquote>

                <p>(This was posted <i>prior</i> to GPT-4. While GPT-1 to 3 <i>were</i> essentially scale-ups of the 2017 transformer decoder, GPT-4 (according to rumors) used a <a href="https://arxiv.org/abs/1701.06538">mixture of experts</a>, a considerable architectural change. Additionally, longer contexts and inference have motivated changes in self-attention.</p>
                <code>To-do: Expound on changes post-GPT-4</code>


                <p></p>
                
            </section>
        </article>
    </div>
</body>
</html>
